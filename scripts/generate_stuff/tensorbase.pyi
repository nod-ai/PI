from __future__ import annotations
import builtins
import warnings
from numbers import Number
from typing import (
    Tuple,
    Optional,
    Any,
    List,
    Dict,
    Callable,
    Union,
    Sequence,
    Generator,
    Literal,
)
import numpy as np

import pi
from torch_mlir.dialects import torch as torch_dialect
from torch_mlir.dialects._ods_common import get_op_result_or_value
from torch_mlir.ir import DenseElementsAttr
from torch_mlir.ir import Value as MLIRValue
from .types_ import (
    dtype as pi_dtype,
    Size,
    device,
    layout,
    Device,
    memory_format,
    contiguous_format,
)
from .dispatcher import dispatch

class TorchTensorWrapper(type):
    def __subclasscheck__(cls, subclass):
        try:
            return subclass._is_pi_tensor()
        except:
            return super(TorchTensorWrapper, cls).__subclasscheck__(subclass)
    @classmethod
    def __instancecheck__(cls, instance):
        try:
            return instance._is_pi_tensor()
        except:
            return False

class ComplexReturnType:
    def __init__(self, name):
        self.name = name

class Tensor(metaclass=TorchTensorWrapper):
    requires_grad: bool
    shape: Size
    data: Tensor
    names: List[str]
    device: device
    dtype: pi_dtype
    layout: layout
    real: Tensor
    imag: Tensor
    T: Tensor
    H: Tensor
    mT: Tensor
    mH: Tensor
    ndim: int
    output_nr: int
    _version: int
    _base: Optional[Tensor]
    _cdata: int
    grad_fn: Any
    _grad_fn: Any
    _grad: Optional[Tensor]
    grad: Optional[Tensor]
    _backward_hooks: Optional[Dict[int, Callable[[Tensor], Optional[Tensor]]]]
    @classmethod
    def _is_pi_tensor(self):
        return True
    @property
    def __class__(self):
        return MLIRValue
    @property
    def type(self):
        return self._value.type
    @property
    def value(self):
        return self._value
    def __init__(self, tensor: MLIRValue):
        self._value = get_op_result_or_value(tensor)
    def __abs__(self) -> Tensor: ...
    def __add__(self, other: Any) -> Tensor: ...
    @dispatch
    def __and__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __and__(self, other: Number) -> Tensor: ...
    @dispatch
    def __and__(self, other: Any) -> Tensor: ...
    def __bool__(self) -> builtins.bool: ...
    def __complex__(self) -> builtins.complex: ...
    def __div__(self, other: Any) -> Tensor: ...
    def __eq__(self, other: Any) -> Tensor: ...  # type: ignore[override]
    def __float__(self) -> builtins.float: ...
    def __floordiv__(self, other: Any) -> Tensor: ...
    def __ge__(self, other: Any) -> Tensor: ...
    def __getitem__(
        self, indices: Union[None, int, slice, Tensor, List, Tuple]
    ) -> Tensor: ...
    def __gt__(self, other: Any) -> Tensor: ...
    def __iadd__(self, other: Any) -> Tensor: ...
    @dispatch
    def __iand__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __iand__(self, other: Number) -> Tensor: ...
    @dispatch
    def __iand__(self, other: Any) -> Tensor: ...
    def __idiv__(self, other: Any) -> Tensor: ...
    def __ifloordiv__(self, other: Any) -> Tensor: ...
    @dispatch
    def __ilshift__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __ilshift__(self, other: Number) -> Tensor: ...
    @dispatch
    def __ilshift__(self, other: Any) -> Tensor: ...
    def __imod__(self, other: Any) -> Tensor: ...
    def __imul__(self, other: Any) -> Tensor: ...
    def __index__(self) -> builtins.int: ...
    def __int__(self) -> builtins.int: ...
    def __invert__(self) -> Tensor: ...
    @dispatch
    def __ior__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __ior__(self, other: Number) -> Tensor: ...
    @dispatch
    def __ior__(self, other: Any) -> Tensor: ...
    @dispatch
    def __irshift__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __irshift__(self, other: Number) -> Tensor: ...
    @dispatch
    def __irshift__(self, other: Any) -> Tensor: ...
    def __isub__(self, other: Any) -> Tensor: ...
    @dispatch
    def __ixor__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __ixor__(self, other: Number) -> Tensor: ...
    @dispatch
    def __ixor__(self, other: Any) -> Tensor: ...
    def __le__(self, other: Any) -> Tensor: ...
    def __long__(self) -> builtins.int: ...
    @dispatch
    def __lshift__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __lshift__(self, other: Number) -> Tensor: ...
    @dispatch
    def __lshift__(self, other: Any) -> Tensor: ...
    def __lt__(self, other: Any) -> Tensor: ...
    def __matmul__(self, other: Any) -> Tensor: ...
    def __mod__(self, other: Any) -> Tensor: ...
    def __mul__(self, other: Any) -> Tensor: ...
    def __ne__(self, other: Any) -> Tensor: ...  # type: ignore[override]
    def __neg__(self) -> Tensor: ...
    def __nonzero__(self) -> builtins.bool: ...
    @dispatch
    def __or__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __or__(self, other: Number) -> Tensor: ...
    @dispatch
    def __or__(self, other: Any) -> Tensor: ...
    def __pow__(self, other: Any) -> Tensor: ...
    def __radd__(self, other: Any) -> Tensor: ...
    def __rand__(self, other: Any) -> Tensor: ...
    def __rfloordiv__(self, other: Any) -> Tensor: ...
    def __rmul__(self, other: Any) -> Tensor: ...
    def __ror__(self, other: Any) -> Tensor: ...
    def __rpow__(self, other: Any) -> Tensor: ...
    @dispatch
    def __rshift__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __rshift__(self, other: Number) -> Tensor: ...
    @dispatch
    def __rshift__(self, other: Any) -> Tensor: ...
    def __rsub__(self, other: Any) -> Tensor: ...
    def __rtruediv__(self, other: Any) -> Tensor: ...
    def __rxor__(self, other: Any) -> Tensor: ...
    def __setitem__(
        self,
        indices: Union[None, int, slice, Tensor, List, Tuple],
        val: Union[Tensor, Number],
    ) -> None: ...
    def __sub__(self, other: Any) -> Tensor: ...
    def __truediv__(self, other: Any) -> Tensor: ...
    @dispatch
    def __xor__(self, other: Tensor) -> Tensor: ...
    @dispatch
    def __xor__(self, other: Number) -> Tensor: ...
    @dispatch
    def __xor__(self, other: Any) -> Tensor: ...
    def _addmm_activation(
        self,
        mat1: Tensor,
        mat2: Tensor,
        *,
        beta: Number = 1,
        alpha: Number = 1,
        use_gelu: bool = False,
    ) -> Tensor: ...
    def _autocast_to_full_precision(
        self, cuda_enabled: bool, cpu_enabled: bool
    ) -> Tensor: ...
    def _autocast_to_reduced_precision(
        self,
        cuda_enabled: bool,
        cpu_enabled: bool,
        cuda_dtype: pi_dtype,
        cpu_dtype: pi_dtype,
    ) -> Tensor: ...
    def _coalesced_(self, coalesced: bool) -> Tensor: ...
    def _conj(self) -> Tensor: ...
    def _conj_physical(self) -> Tensor: ...
    def _dimI(self) -> int: ...
    def _dimV(self) -> int: ...
    def _indices(self) -> Tensor: ...
    def _is_view(self) -> bool: ...
    def _is_zerotensor(self) -> bool: ...
    def _make_subclass(
        cls,
        data: Tensor,
        require_grad: bool = False,
        dispatch_strides: bool = False,
        dispatch_device: bool = False,
        device_for_backend_keys: Optional[device] = None,
    ) -> Tensor: ...
    def _neg_view(self) -> Tensor: ...
    def _nested_tensor_size(self) -> Tensor: ...
    def _nnz(self) -> int: ...
    def _to_dense(self, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    def _values(self) -> Tensor: ...
    def abs(self) -> Tensor: ...
    def abs_(self) -> Tensor: ...
    def absolute(self) -> Tensor: ...
    def absolute_(self) -> Tensor: ...
    def acos(self) -> Tensor: ...
    def acos_(self) -> Tensor: ...
    def acosh(self) -> Tensor: ...
    def acosh_(self) -> Tensor: ...
    def add(
        self,
        other: Union[Tensor, Number],
        *,
        alpha: Optional[Number] = 1,
        out: Optional[Tensor] = None,
    ) -> Tensor: ...
    def add_(
        self, other: Union[Tensor, Number], *, alpha: Optional[Number] = 1
    ) -> Tensor: ...
    def addbmm(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addbmm_(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addcdiv(
        self, tensor1: Tensor, tensor2: Tensor, *, value: Number = 1
    ) -> Tensor: ...
    def addcdiv_(
        self, tensor1: Tensor, tensor2: Tensor, *, value: Number = 1
    ) -> Tensor: ...
    def addcmul(
        self, tensor1: Tensor, tensor2: Tensor, *, value: Number = 1
    ) -> Tensor: ...
    def addcmul_(
        self, tensor1: Tensor, tensor2: Tensor, *, value: Number = 1
    ) -> Tensor: ...
    def addmm(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addmm_(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addmv(
        self, mat: Tensor, vec: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addmv_(
        self, mat: Tensor, vec: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addr(
        self, vec1: Tensor, vec2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def addr_(
        self, vec1: Tensor, vec2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def adjoint(self) -> Tensor: ...
    def align_as(self, other: Tensor) -> Tensor: ...
    @dispatch
    def align_to(
        self, order: Sequence[Union[str, ellipsis, None]], ellipsis_idx: int
    ) -> Tensor: ...
    @dispatch
    def align_to(self, names: Sequence[Union[str, ellipsis, None]]) -> Tensor: ...
    @dispatch
    def all(self) -> Tensor: ...
    @dispatch
    def all(self, dim: int, keepdim: bool = False) -> Tensor: ...
    @dispatch
    def all(self, dim: Union[str, ellipsis, None], keepdim: bool = False) -> Tensor: ...
    def allclose(
        self,
        other: Tensor,
        rtol: float = 1e-05,
        atol: float = 1e-08,
        equal_nan: bool = False,
    ) -> bool: ...
    def amax(self, dim: Union[int, Size] = (), keepdim: bool = False) -> Tensor: ...
    def amin(self, dim: Union[int, Size] = (), keepdim: bool = False) -> Tensor: ...
    def aminmax(
        self, *, dim: Optional[int] = None, keepdim: bool = False
    ) -> ComplexReturnType("aminmax"): ...
    def angle(self) -> Tensor: ...
    @dispatch
    def any(self) -> Tensor: ...
    @dispatch
    def any(self, dim: int, keepdim: bool = False) -> Tensor: ...
    @dispatch
    def any(self, dim: Union[str, ellipsis, None], keepdim: bool = False) -> Tensor: ...
    def apply_(self, callable: Callable) -> Tensor: ...
    def arccos(self) -> Tensor: ...
    def arccos_(self) -> Tensor: ...
    def arccosh(self) -> Tensor: ...
    def arccosh_(self) -> Tensor: ...
    def arcsin(self) -> Tensor: ...
    def arcsin_(self) -> Tensor: ...
    def arcsinh(self) -> Tensor: ...
    def arcsinh_(self) -> Tensor: ...
    def arctan(self) -> Tensor: ...
    def arctan2(self, other: Tensor) -> Tensor: ...
    def arctan2_(self, other: Tensor) -> Tensor: ...
    def arctan_(self) -> Tensor: ...
    def arctanh(self) -> Tensor: ...
    def arctanh_(self) -> Tensor: ...
    def argmax(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    def argmin(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    @dispatch
    def argsort(
        self, *, stable: bool, dim: int = -1, descending: bool = False
    ) -> Tensor: ...
    @dispatch
    def argsort(self, dim: int = -1, descending: bool = False) -> Tensor: ...
    @dispatch
    def argsort(
        self, dim: Union[str, ellipsis, None], descending: bool = False
    ) -> Tensor: ...
    def argwhere(self) -> Tensor: ...
    def as_strided(
        self,
        size: List[int],
        stride: List[int],
        storage_offset: Optional[int] = None,
    ) -> Tensor: ...
    def as_strided_(
        self,
        size: List[int],
        stride: List[int],
        storage_offset: Optional[int] = None,
    ) -> Tensor: ...
    def as_strided_scatter(
        self,
        src: Tensor,
        size: List[int],
        stride: List[int],
        storage_offset: Optional[int] = None,
    ) -> Tensor: ...
    def asin(self) -> Tensor: ...
    def asin_(self) -> Tensor: ...
    def asinh(self) -> Tensor: ...
    def asinh_(self) -> Tensor: ...
    def atan(self) -> Tensor: ...
    def atan2(self, other: Tensor) -> Tensor: ...
    def atan2_(self, other: Tensor) -> Tensor: ...
    def atan_(self) -> Tensor: ...
    def atanh(self) -> Tensor: ...
    def atanh_(self) -> Tensor: ...
    def baddbmm(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    def baddbmm_(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    @dispatch
    def bernoulli(self, *, generator: Optional[Generator] = None) -> Tensor: ...
    @dispatch
    def bernoulli(
        self, p: float, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def bernoulli_(
        self, p: Tensor, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def bernoulli_(
        self, p: float = 0.5, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    def bfloat16(self) -> Tensor: ...
    def bincount(
        self, weights: Optional[Tensor] = None, minlength: int = 0
    ) -> Tensor: ...
    @dispatch
    def bitwise_and(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_and(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_and_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_and_(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_left_shift(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_left_shift(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_left_shift_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_left_shift_(self, other: Number) -> Tensor: ...
    def bitwise_not(self) -> Tensor: ...
    def bitwise_not_(self) -> Tensor: ...
    @dispatch
    def bitwise_or(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_or(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_or_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_or_(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_right_shift(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_right_shift(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_right_shift_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_right_shift_(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_xor(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_xor(self, other: Number) -> Tensor: ...
    @dispatch
    def bitwise_xor_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def bitwise_xor_(self, other: Number) -> Tensor: ...
    def bmm(self, mat2: Tensor) -> Tensor: ...
    def bool(self) -> Tensor: ...
    @dispatch
    def broadcast_to(self, size: List[int]) -> Tensor: ...
    @dispatch
    def broadcast_to(self, *size: int) -> Tensor: ...
    def byte(self) -> Tensor: ...
    def cauchy_(
        self,
        median: float = 0,
        sigma: float = 1,
        *,
        generator: Optional[Generator] = None,
    ) -> Tensor: ...
    def ccol_indices(self) -> Tensor: ...
    def ceil(self) -> Tensor: ...
    def ceil_(self) -> Tensor: ...
    def chalf(self, *, memory_format: Optional[memory_format] = None) -> Tensor: ...
    def char(self) -> Tensor: ...
    def cholesky(self, upper: bool = False) -> Tensor: ...
    def cholesky_inverse(self, upper: bool = False) -> Tensor: ...
    def cholesky_solve(self, input2: Tensor, upper: bool = False) -> Tensor: ...
    def chunk(self, chunks: int, dim: int = 0) -> List[Tensor]: ...
    @dispatch
    def clamp(
        self, min: Optional[Tensor] = None, max: Optional[Tensor] = None
    ) -> Tensor: ...
    @dispatch
    def clamp(
        self, min: Optional[Number] = None, max: Optional[Number] = None
    ) -> Tensor: ...
    @dispatch
    def clamp_(
        self, min: Optional[Tensor] = None, max: Optional[Tensor] = None
    ) -> Tensor: ...
    @dispatch
    def clamp_(
        self, min: Optional[Number] = None, max: Optional[Number] = None
    ) -> Tensor: ...
    @dispatch
    def clamp_max(self, max: Tensor) -> Tensor: ...
    @dispatch
    def clamp_max(self, max: Number) -> Tensor: ...
    @dispatch
    def clamp_max_(self, max: Tensor) -> Tensor: ...
    @dispatch
    def clamp_max_(self, max: Number) -> Tensor: ...
    @dispatch
    def clamp_min(self, min: Tensor) -> Tensor: ...
    @dispatch
    def clamp_min(self, min: Number) -> Tensor: ...
    @dispatch
    def clamp_min_(self, min: Tensor) -> Tensor: ...
    @dispatch
    def clamp_min_(self, min: Number) -> Tensor: ...
    @dispatch
    def clip(
        self, min: Optional[Tensor] = None, max: Optional[Tensor] = None
    ) -> Tensor: ...
    @dispatch
    def clip(
        self, min: Optional[Number] = None, max: Optional[Number] = None
    ) -> Tensor: ...
    @dispatch
    def clip_(
        self, min: Optional[Tensor] = None, max: Optional[Tensor] = None
    ) -> Tensor: ...
    @dispatch
    def clip_(
        self, min: Optional[Number] = None, max: Optional[Number] = None
    ) -> Tensor: ...
    def clone(self, *, memory_format: Optional[memory_format] = None) -> Tensor: ...
    def coalesce(self) -> Tensor: ...
    def col_indices(self) -> Tensor: ...
    def conj(self) -> Tensor: ...
    def conj_physical(self) -> Tensor: ...
    def conj_physical_(self) -> Tensor: ...
    def contiguous(self, memory_format=contiguous_format) -> Tensor: ...
    def copy_(self, src: Tensor, non_blocking: bool = False) -> Tensor: ...
    @dispatch
    def copysign(self, other: Tensor) -> Tensor: ...
    @dispatch
    def copysign(self, other: Number) -> Tensor: ...
    @dispatch
    def copysign_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def copysign_(self, other: Number) -> Tensor: ...
    def corrcoef(self) -> Tensor: ...
    def cos(self) -> Tensor: ...
    def cos_(self) -> Tensor: ...
    def cosh(self) -> Tensor: ...
    def cosh_(self) -> Tensor: ...
    @dispatch
    def count_nonzero(self, dim: Optional[int] = None) -> Tensor: ...
    @dispatch
    def count_nonzero(self, dim: Size) -> Tensor: ...
    @dispatch
    def count_nonzero(self, *dim: int) -> Tensor: ...
    def cov(
        self,
        *,
        correction: int = 1,
        fweights: Optional[Tensor] = None,
        aweights: Optional[Tensor] = None,
    ) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def cross(self, other: Tensor, dim: Optional[int] = None) -> Tensor: ...
    def crow_indices(self) -> Tensor: ...
    def cuda(
        self,
        device: Optional[Union[device, int, str]] = None,
        non_blocking: bool = False,
    ) -> Tensor: ...
    @dispatch
    def cummax(self, dim: int) -> ComplexReturnType("cummax"): ...
    @dispatch
    def cummax(
        self, dim: Union[str, ellipsis, None]
    ) -> ComplexReturnType("cummax"): ...
    @dispatch
    def cummin(self, dim: int) -> ComplexReturnType("cummin"): ...
    @dispatch
    def cummin(
        self, dim: Union[str, ellipsis, None]
    ) -> ComplexReturnType("cummin"): ...
    @dispatch
    def cumprod(self, dim: int, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def cumprod(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    @dispatch
    def cumprod_(self, dim: int, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def cumprod_(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    @dispatch
    def cumsum(self, dim: int, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def cumsum(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    @dispatch
    def cumsum_(self, dim: int, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def cumsum_(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    def data_ptr(self) -> int: ...
    def deg2rad(self) -> Tensor: ...
    def deg2rad_(self) -> Tensor: ...
    def dense_dim(self) -> int: ...
    def dequantize(self) -> Tensor: ...
    def det(self) -> Tensor: ...
    def detach(self) -> Tensor: ...
    def detach_(self) -> Tensor: ...
    def diag(self, diagonal: int = 0) -> Tensor: ...
    def diag_embed(self, offset: int = 0, dim1: int = -2, dim2: int = -1) -> Tensor: ...
    def diagflat(self, offset: int = 0) -> Tensor: ...
    @dispatch
    def diagonal(
        self,
        *,
        outdim: Union[str, ellipsis, None],
        dim1: Union[str, ellipsis, None],
        dim2: Union[str, ellipsis, None],
        offset: int = 0,
    ) -> Tensor: ...
    @dispatch
    def diagonal(self, offset: int = 0, dim1: int = 0, dim2: int = 1) -> Tensor: ...
    def diagonal_scatter(
        self, src: Tensor, offset: int = 0, dim1: int = 0, dim2: int = 1
    ) -> Tensor: ...
    def diff(
        self,
        n: int = 1,
        dim: int = -1,
        prepend: Optional[Tensor] = None,
        append: Optional[Tensor] = None,
    ) -> Tensor: ...
    def digamma(self) -> Tensor: ...
    def digamma_(self) -> Tensor: ...
    def dim(self) -> int: ...
    def dist(self, other: Tensor, p: Number = 2) -> Tensor: ...
    def div(
        self, other: Union[Tensor, Number], *, rounding_mode: Optional[str] = None
    ) -> Tensor: ...
    def div_(
        self, other: Union[Tensor, Number], *, rounding_mode: Optional[str] = None
    ) -> Tensor: ...
    @dispatch
    def divide(self, other: Tensor) -> Tensor: ...
    @dispatch
    def divide(self, other: Tensor, *, rounding_mode: Optional[str]) -> Tensor: ...
    @dispatch
    def divide(self, other: Number, *, rounding_mode: Optional[str]) -> Tensor: ...
    @dispatch
    def divide(self, other: Number) -> Tensor: ...
    @dispatch
    def divide_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def divide_(self, other: Tensor, *, rounding_mode: Optional[str]) -> Tensor: ...
    @dispatch
    def divide_(self, other: Number, *, rounding_mode: Optional[str]) -> Tensor: ...
    @dispatch
    def divide_(self, other: Number) -> Tensor: ...
    def dot(self, tensor: Tensor) -> Tensor: ...
    def double(self) -> Tensor: ...
    @dispatch
    def dsplit(self, sections: int) -> List[Tensor]: ...
    @dispatch
    def dsplit(self, indices: Size) -> List[Tensor]: ...
    @dispatch
    def dsplit(self, *indices: int) -> List[Tensor]: ...
    def element_size(self) -> int: ...
    @dispatch
    def eq(self, other: Tensor) -> Tensor: ...
    @dispatch
    def eq(self, other: Number) -> Tensor: ...
    @dispatch
    def eq_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def eq_(self, other: Number) -> Tensor: ...
    def equal(self, other: Tensor) -> bool: ...
    def erf(self) -> Tensor: ...
    def erf_(self) -> Tensor: ...
    def erfc(self) -> Tensor: ...
    def erfc_(self) -> Tensor: ...
    def erfinv(self) -> Tensor: ...
    def erfinv_(self) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def exp2(self) -> Tensor: ...
    def exp2_(self) -> Tensor: ...
    def exp_(self) -> Tensor: ...
    @dispatch
    def expand(self, size: List[int], *, implicit: bool = False) -> Tensor: ...
    @dispatch
    def expand(self, *size: int, implicit: bool = False) -> Tensor: ...
    def expand_as(self, other: Tensor) -> Tensor: ...
    def expm1(self) -> Tensor: ...
    def expm1_(self) -> Tensor: ...
    def exponential_(
        self, lambd: float = 1, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def fill_(self, value: Tensor) -> Tensor: ...
    @dispatch
    def fill_(self, value: Number) -> Tensor: ...
    def fill_diagonal_(self, fill_value: Number, wrap: bool = False) -> Tensor: ...
    def fix(self) -> Tensor: ...
    def fix_(self) -> Tensor: ...
    @dispatch
    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> Tensor: ...
    @dispatch
    def flatten(
        self, start_dim: int, end_dim: int, out_dim: Union[str, ellipsis, None]
    ) -> Tensor: ...
    @dispatch
    def flatten(
        self,
        start_dim: Union[str, ellipsis, None],
        end_dim: Union[str, ellipsis, None],
        out_dim: Union[str, ellipsis, None],
    ) -> Tensor: ...
    @dispatch
    def flatten(
        self,
        dims: Sequence[Union[str, ellipsis, None]],
        out_dim: Union[str, ellipsis, None],
    ) -> Tensor: ...
    @dispatch
    def flip(self, dims: Size) -> Tensor: ...
    @dispatch
    def flip(self, *dims: int) -> Tensor: ...
    def fliplr(self) -> Tensor: ...
    def flipud(self) -> Tensor: ...
    def float(self) -> Tensor: ...
    @dispatch
    def float_power(self, exponent: Tensor) -> Tensor: ...
    @dispatch
    def float_power(self, exponent: Number) -> Tensor: ...
    @dispatch
    def float_power_(self, exponent: Tensor) -> Tensor: ...
    @dispatch
    def float_power_(self, exponent: Number) -> Tensor: ...
    def floor(self) -> Tensor: ...
    def floor_(self) -> Tensor: ...
    def floor_divide(
        self, other: Union[Tensor, Number], *, out: Optional[Tensor] = None
    ) -> Tensor: ...
    def floor_divide_(self, other: Union[Tensor, Number]) -> Tensor: ...
    def fmax(self, other: Tensor) -> Tensor: ...
    def fmin(self, other: Tensor) -> Tensor: ...
    @dispatch
    def fmod(self, other: Tensor) -> Tensor: ...
    @dispatch
    def fmod(self, other: Number) -> Tensor: ...
    @dispatch
    def fmod_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def fmod_(self, other: Number) -> Tensor: ...
    def frac(self) -> Tensor: ...
    def frac_(self) -> Tensor: ...
    def frexp(self) -> ComplexReturnType("frexp"): ...
    @dispatch
    def gather(
        self, dim: int, index: Tensor, *, sparse_grad: bool = False
    ) -> Tensor: ...
    @dispatch
    def gather(
        self,
        dim: Union[str, ellipsis, None],
        index: Tensor,
        *,
        sparse_grad: bool = False,
    ) -> Tensor: ...
    def gcd(self, other: Tensor) -> Tensor: ...
    def gcd_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def ge(self, other: Tensor) -> Tensor: ...
    @dispatch
    def ge(self, other: Number) -> Tensor: ...
    @dispatch
    def ge_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def ge_(self, other: Number) -> Tensor: ...
    def geometric_(
        self, p: float, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    def geqrf(self) -> ComplexReturnType("geqrf"): ...
    def ger(self, vec2: Tensor) -> Tensor: ...
    def get_device(self) -> int: ...
    @dispatch
    def greater(self, other: Tensor) -> Tensor: ...
    @dispatch
    def greater(self, other: Number) -> Tensor: ...
    @dispatch
    def greater_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def greater_(self, other: Number) -> Tensor: ...
    @dispatch
    def greater_equal(self, other: Tensor) -> Tensor: ...
    @dispatch
    def greater_equal(self, other: Number) -> Tensor: ...
    @dispatch
    def greater_equal_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def greater_equal_(self, other: Number) -> Tensor: ...
    @dispatch
    def gt(self, other: Tensor) -> Tensor: ...
    @dispatch
    def gt(self, other: Number) -> Tensor: ...
    @dispatch
    def gt_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def gt_(self, other: Number) -> Tensor: ...
    def half(self) -> Tensor: ...
    def hardshrink(self, lambd: Number = 0.5) -> Tensor: ...
    def has_names(self) -> bool: ...
    def heaviside(self, values: Tensor) -> Tensor: ...
    def heaviside_(self, values: Tensor) -> Tensor: ...
    def histc(self, bins: int = 100, min: Number = 0, max: Number = 0) -> Tensor: ...
    @dispatch
    def histogram(
        self, bins: Tensor, *, weight: Optional[Tensor] = None, density: bool = False
    ) -> ComplexReturnType("histogram"): ...
    @dispatch
    def histogram(
        self,
        bins: int = 100,
        *,
        range: Optional[Sequence[float]] = None,
        weight: Optional[Tensor] = None,
        density: bool = False,
    ) -> ComplexReturnType("histogram"): ...
    @dispatch
    def hsplit(self, sections: int) -> List[Tensor]: ...
    @dispatch
    def hsplit(self, indices: Size) -> List[Tensor]: ...
    @dispatch
    def hsplit(self, *indices: int) -> List[Tensor]: ...
    def hypot(self, other: Tensor) -> Tensor: ...
    def hypot_(self, other: Tensor) -> Tensor: ...
    def i0(self) -> Tensor: ...
    def i0_(self) -> Tensor: ...
    def igamma(self, other: Tensor) -> Tensor: ...
    def igamma_(self, other: Tensor) -> Tensor: ...
    def igammac(self, other: Tensor) -> Tensor: ...
    def igammac_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def index_add(
        self, dim: int, index: Tensor, source: Tensor, *, alpha: Number = 1
    ) -> Tensor: ...
    @dispatch
    def index_add(
        self,
        dim: Union[str, ellipsis, None],
        index: Tensor,
        source: Tensor,
        *,
        alpha: Number = 1,
    ) -> Tensor: ...
    def index_add_(
        self, dim: int, index: Tensor, source: Tensor, *, alpha: Number = 1
    ) -> Tensor: ...
    @dispatch
    def index_copy(self, dim: int, index: Tensor, source: Tensor) -> Tensor: ...
    @dispatch
    def index_copy(
        self, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor
    ) -> Tensor: ...
    @dispatch
    def index_copy_(self, dim: int, index: Tensor, source: Tensor) -> Tensor: ...
    @dispatch
    def index_copy_(
        self, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor
    ) -> Tensor: ...
    @dispatch
    def index_fill(self, dim: int, index: Tensor, value: Tensor) -> Tensor: ...
    @dispatch
    def index_fill(
        self, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor
    ) -> Tensor: ...
    @dispatch
    def index_fill(self, dim: int, index: Tensor, value: Number) -> Tensor: ...
    @dispatch
    def index_fill(
        self, dim: Union[str, ellipsis, None], index: Tensor, value: Number
    ) -> Tensor: ...
    @dispatch
    def index_fill_(self, dim: int, index: Tensor, value: Tensor) -> Tensor: ...
    @dispatch
    def index_fill_(
        self, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor
    ) -> Tensor: ...
    @dispatch
    def index_fill_(self, dim: int, index: Tensor, value: Number) -> Tensor: ...
    @dispatch
    def index_fill_(
        self, dim: Union[str, ellipsis, None], index: Tensor, value: Number
    ) -> Tensor: ...
    def index_put(
        self,
        indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]],
        values: Tensor,
        accumulate: bool = False,
    ) -> Tensor: ...
    def index_put_(
        self,
        indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]],
        values: Tensor,
        accumulate: bool = False,
    ) -> Tensor: ...
    def index_reduce(
        self,
        dim: int,
        index: Tensor,
        source: Tensor,
        reduce: str,
        *,
        include_self: bool = True,
    ) -> Tensor: ...
    def index_reduce_(
        self,
        dim: int,
        index: Tensor,
        source: Tensor,
        reduce: str,
        *,
        include_self: bool = True,
    ) -> Tensor: ...
    @dispatch
    def index_select(self, dim: int, index: Tensor) -> Tensor: ...
    @dispatch
    def index_select(
        self, dim: Union[str, ellipsis, None], index: Tensor
    ) -> Tensor: ...
    def indices(self) -> Tensor: ...
    def inner(self, other: Tensor) -> Tensor: ...
    def int(self) -> Tensor: ...
    def int_repr(self) -> Tensor: ...
    def inverse(self) -> Tensor: ...
    def is_coalesced(self) -> bool: ...
    def is_complex(self) -> bool: ...
    def is_conj(self) -> bool: ...
    def is_contiguous(self, memory_format=contiguous_format) -> bool: ...
    is_cuda: bool
    def is_distributed(self) -> bool: ...
    def is_floating_point(self) -> bool: ...
    def is_inference(self) -> bool: ...
    is_ipu: bool
    is_leaf: bool
    is_meta: bool
    is_mkldnn: bool
    is_mps: bool
    def is_neg(self) -> bool: ...
    is_nested: bool
    def is_nonzero(self) -> bool: ...
    is_ort: bool
    def is_pinned(self, device: Optional[Union[device, str, None]] = None) -> bool: ...
    is_quantized: bool
    def is_same_size(self, other: Tensor) -> bool: ...
    def is_set_to(self, tensor: Tensor) -> bool: ...
    def is_signed(self) -> bool: ...
    is_sparse: bool
    is_sparse_csr: bool
    is_vulkan: bool
    def isclose(
        self,
        other: Tensor,
        rtol: float = 1e-05,
        atol: float = 1e-08,
        equal_nan: bool = False,
    ) -> Tensor: ...
    def isfinite(self) -> Tensor: ...
    def isinf(self) -> Tensor: ...
    def isnan(self) -> Tensor: ...
    def isneginf(self) -> Tensor: ...
    def isposinf(self) -> Tensor: ...
    def isreal(self) -> Tensor: ...
    def istft(
        self,
        n_fft: int,
        hop_length: Optional[int] = None,
        win_length: Optional[int] = None,
        window: Optional[Tensor] = None,
        center: bool = True,
        normalized: bool = False,
        onesided: Optional[bool] = None,
        length: Optional[int] = None,
        return_complex: bool = False,
    ) -> Tensor: ...
    def item(self) -> Number: ...
    def kron(self, other: Tensor) -> Tensor: ...
    @dispatch
    def kthvalue(
        self, k: int, dim: int = -1, keepdim: bool = False
    ) -> ComplexReturnType("kthvalue"): ...
    @dispatch
    def kthvalue(
        self, k: int, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("kthvalue"): ...
    def lcm(self, other: Tensor) -> Tensor: ...
    def lcm_(self, other: Tensor) -> Tensor: ...
    def ldexp(self, other: Tensor) -> Tensor: ...
    def ldexp_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def le(self, other: Tensor) -> Tensor: ...
    @dispatch
    def le(self, other: Number) -> Tensor: ...
    @dispatch
    def le_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def le_(self, other: Number) -> Tensor: ...
    @dispatch
    def lerp(self, end: Tensor, weight: Tensor) -> Tensor: ...
    @dispatch
    def lerp(self, end: Tensor, weight: Number) -> Tensor: ...
    @dispatch
    def lerp_(self, end: Tensor, weight: Tensor) -> Tensor: ...
    @dispatch
    def lerp_(self, end: Tensor, weight: Number) -> Tensor: ...
    @dispatch
    def less(self, other: Tensor) -> Tensor: ...
    @dispatch
    def less(self, other: Number) -> Tensor: ...
    @dispatch
    def less_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def less_(self, other: Number) -> Tensor: ...
    @dispatch
    def less_equal(self, other: Tensor) -> Tensor: ...
    @dispatch
    def less_equal(self, other: Number) -> Tensor: ...
    @dispatch
    def less_equal_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def less_equal_(self, other: Number) -> Tensor: ...
    def lgamma(self) -> Tensor: ...
    def lgamma_(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def log10(self) -> Tensor: ...
    def log10_(self) -> Tensor: ...
    def log1p(self) -> Tensor: ...
    def log1p_(self) -> Tensor: ...
    def log2(self) -> Tensor: ...
    def log2_(self) -> Tensor: ...
    def log_(self) -> Tensor: ...
    def log_normal_(
        self, mean: float = 1, std: float = 2, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def log_softmax(self, dim: int, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def log_softmax(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    def logaddexp(self, other: Tensor) -> Tensor: ...
    def logaddexp2(self, other: Tensor) -> Tensor: ...
    @dispatch
    def logcumsumexp(self, dim: int) -> Tensor: ...
    @dispatch
    def logcumsumexp(self, dim: Union[str, ellipsis, None]) -> Tensor: ...
    def logdet(self) -> Tensor: ...
    def logical_and(self, other: Tensor) -> Tensor: ...
    def logical_and_(self, other: Tensor) -> Tensor: ...
    def logical_not(self) -> Tensor: ...
    def logical_not_(self) -> Tensor: ...
    def logical_or(self, other: Tensor) -> Tensor: ...
    def logical_or_(self, other: Tensor) -> Tensor: ...
    def logical_xor(self, other: Tensor) -> Tensor: ...
    def logical_xor_(self, other: Tensor) -> Tensor: ...
    def logit(self, eps: Optional[float] = None) -> Tensor: ...
    def logit_(self, eps: Optional[float] = None) -> Tensor: ...
    @dispatch
    def logsumexp(self, dim: Union[int, Size], keepdim: bool = False) -> Tensor: ...
    @dispatch
    def logsumexp(
        self, dim: Sequence[Union[str, ellipsis, None]], keepdim: bool = False
    ) -> Tensor: ...
    def long(self) -> Tensor: ...
    @dispatch
    def lt(self, other: Tensor) -> Tensor: ...
    @dispatch
    def lt(self, other: Number) -> Tensor: ...
    @dispatch
    def lt_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def lt_(self, other: Number) -> Tensor: ...
    def lu_solve(self, LU_data: Tensor, LU_pivots: Tensor) -> Tensor: ...
    def map2_(self, x: Tensor, y: Tensor, callable: Callable) -> Tensor: ...
    def map_(self, tensor: Tensor, callable: Callable) -> Tensor: ...
    @dispatch
    def masked_fill(self, mask: Tensor, value: Tensor) -> Tensor: ...
    @dispatch
    def masked_fill(self, mask: Tensor, value: Number) -> Tensor: ...
    @dispatch
    def masked_fill_(self, mask: Tensor, value: Tensor) -> Tensor: ...
    @dispatch
    def masked_fill_(self, mask: Tensor, value: Number) -> Tensor: ...
    def masked_scatter(self, mask: Tensor, source: Tensor) -> Tensor: ...
    def masked_scatter_(self, mask: Tensor, source: Tensor) -> Tensor: ...
    def masked_select(self, mask: Tensor) -> Tensor: ...
    def matmul(self, other: Tensor) -> Tensor: ...
    def matrix_exp(self) -> Tensor: ...
    def matrix_power(self, n: int) -> Tensor: ...
    @dispatch
    def max(self) -> Tensor: ...
    @dispatch
    def max(self, other: Tensor) -> Tensor: ...
    @dispatch
    def max(self, dim: int, keepdim: bool = False) -> ComplexReturnType("max"): ...
    @dispatch
    def max(
        self, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("max"): ...
    def maximum(self, other: Tensor) -> Tensor: ...
    @dispatch
    def mean(self, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def mean(
        self,
        dim: Optional[Union[int, Size]],
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def mean(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def median(self) -> Tensor: ...
    @dispatch
    def median(
        self, dim: int, keepdim: bool = False
    ) -> ComplexReturnType("median"): ...
    @dispatch
    def median(
        self, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("median"): ...
    @dispatch
    def min(self) -> Tensor: ...
    @dispatch
    def min(self, other: Tensor) -> Tensor: ...
    @dispatch
    def min(self, dim: int, keepdim: bool = False) -> ComplexReturnType("min"): ...
    @dispatch
    def min(
        self, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("min"): ...
    def minimum(self, other: Tensor) -> Tensor: ...
    def mm(self, mat2: Tensor) -> Tensor: ...
    @dispatch
    def mode(
        self, dim: int = -1, keepdim: bool = False
    ) -> ComplexReturnType("mode"): ...
    @dispatch
    def mode(
        self, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("mode"): ...
    @dispatch
    def moveaxis(self, source: int, destination: int) -> Tensor: ...
    @dispatch
    def moveaxis(self, source: Size, destination: Size) -> Tensor: ...
    @dispatch
    def movedim(self, source: int, destination: int) -> Tensor: ...
    @dispatch
    def movedim(self, source: Size, destination: Size) -> Tensor: ...
    def msort(self) -> Tensor: ...
    def mul(
        self, other: Union[Tensor, Number], *, out: Optional[Tensor] = None
    ) -> Tensor: ...
    def mul_(self, other: Union[Tensor, Number]) -> Tensor: ...
    def multinomial(
        self,
        num_samples: int,
        replacement: bool = False,
        *,
        generator: Optional[Generator] = None,
    ) -> Tensor: ...
    @dispatch
    def multiply(self, other: Tensor) -> Tensor: ...
    @dispatch
    def multiply(self, other: Number) -> Tensor: ...
    @dispatch
    def multiply_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def multiply_(self, other: Number) -> Tensor: ...
    def mv(self, vec: Tensor) -> Tensor: ...
    def mvlgamma(self, p: int) -> Tensor: ...
    def mvlgamma_(self, p: int) -> Tensor: ...
    def nan_to_num(
        self,
        nan: Optional[float] = None,
        posinf: Optional[float] = None,
        neginf: Optional[float] = None,
    ) -> Tensor: ...
    def nan_to_num_(
        self,
        nan: Optional[float] = None,
        posinf: Optional[float] = None,
        neginf: Optional[float] = None,
    ) -> Tensor: ...
    def nanmean(
        self,
        dim: Optional[Union[int, Size]] = None,
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def nanmedian(self) -> Tensor: ...
    @dispatch
    def nanmedian(
        self, dim: int, keepdim: bool = False
    ) -> ComplexReturnType("nanmedian"): ...
    @dispatch
    def nanmedian(
        self, dim: Union[str, ellipsis, None], keepdim: bool = False
    ) -> ComplexReturnType("nanmedian"): ...
    @dispatch
    def nanquantile(
        self,
        q: Tensor,
        dim: Optional[int] = None,
        keepdim: bool = False,
        *,
        interpolation: str = "linear",
    ) -> Tensor: ...
    @dispatch
    def nanquantile(
        self,
        q: float,
        dim: Optional[int] = None,
        keepdim: bool = False,
        *,
        interpolation: str = "linear",
    ) -> Tensor: ...
    def nansum(
        self,
        dim: Optional[Union[int, Size]] = None,
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def narrow(self, dim: int, start: Tensor, length: int) -> Tensor: ...
    @dispatch
    def narrow(self, dim: int, start: int, length: int) -> Tensor: ...
    def narrow_copy(self, dim: int, start: int, length: int) -> Tensor: ...
    def ndimension(self) -> int: ...
    @dispatch
    def ne(self, other: Tensor) -> Tensor: ...
    @dispatch
    def ne(self, other: Number) -> Tensor: ...
    @dispatch
    def ne_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def ne_(self, other: Number) -> Tensor: ...
    def neg(self) -> Tensor: ...
    def neg_(self) -> Tensor: ...
    def negative(self) -> Tensor: ...
    def negative_(self) -> Tensor: ...
    def nelement(self) -> int: ...
    @dispatch
    def new(self, *args: Any, device: Device = None) -> Tensor: ...
    @dispatch
    def new(self, other: Tensor) -> Tensor: ...
    @dispatch
    def new(self, size: Size, *, device: Device = None) -> Tensor: ...
    @dispatch
    def new_empty(
        self,
        size: List[int],
        *,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    @dispatch
    def new_empty(
        self,
        *size: int,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    def new_empty_strided(
        self,
        size: List[int],
        stride: List[int],
        *,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    def new_full(
        self,
        size: List[int],
        fill_value: Number,
        *,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    @dispatch
    def new_ones(
        self,
        size: Size,
        dtype: Optional[pi_dtype] = None,
        device: Device = None,
        requires_grad: bool = False,
    ) -> Tensor: ...
    @dispatch
    def new_ones(
        self,
        size: List[int],
        *,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    @dispatch
    def new_ones(
        self,
        *size: int,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    def new_tensor(
        self,
        data: Any,
        dtype: Optional[pi_dtype] = None,
        device: Device = None,
        requires_grad: bool = False,
    ) -> Tensor: ...
    @dispatch
    def new_zeros(
        self,
        size: List[int],
        *,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    @dispatch
    def new_zeros(
        self,
        *size: int,
        dtype: Optional[pi_dtype] = None,
        layout: Optional[layout] = None,
        device: Optional[Union[device, str, None]] = None,
        pin_memory: Optional[bool] = False,
        requires_grad: Optional[bool] = False,
    ) -> Tensor: ...
    def nextafter(self, other: Tensor) -> Tensor: ...
    def nextafter_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def nonzero(self, *, as_tuple: Literal[False] = False) -> Tensor: ...
    @dispatch
    def nonzero(self, *, as_tuple: Literal[True]) -> Tuple[Tensor, ...]: ...
    def normal_(
        self, mean: float = 0, std: float = 1, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def not_equal(self, other: Tensor) -> Tensor: ...
    @dispatch
    def not_equal(self, other: Number) -> Tensor: ...
    @dispatch
    def not_equal_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def not_equal_(self, other: Number) -> Tensor: ...
    def numel(self) -> int: ...
    def numpy(self, *, force: bool = False) -> Any: ...
    def orgqr(self, input2: Tensor) -> Tensor: ...
    def ormqr(
        self, input2: Tensor, input3: Tensor, left: bool = True, transpose: bool = False
    ) -> Tensor: ...
    def outer(self, vec2: Tensor) -> Tensor: ...
    @dispatch
    def permute(self, dims: Size) -> Tensor: ...
    @dispatch
    def permute(self, *dims: int) -> Tensor: ...
    def pin_memory(
        self, device: Optional[Union[device, str, None]] = None
    ) -> Tensor: ...
    def pinverse(self, rcond: float = 1e-15) -> Tensor: ...
    def polygamma(self, n: int) -> Tensor: ...
    def polygamma_(self, n: int) -> Tensor: ...
    def positive(self) -> Tensor: ...
    @dispatch
    def pow(self, exponent: Tensor) -> Tensor: ...
    @dispatch
    def pow(self, exponent: Number) -> Tensor: ...
    @dispatch
    def pow_(self, exponent: Tensor) -> Tensor: ...
    @dispatch
    def pow_(self, exponent: Number) -> Tensor: ...
    def prelu(self, weight: Tensor) -> Tensor: ...
    @dispatch
    def prod(self, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def prod(
        self, dim: int, keepdim: bool = False, *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    @dispatch
    def prod(
        self,
        dim: Union[str, ellipsis, None],
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    def put(
        self, index: Tensor, source: Tensor, accumulate: bool = False
    ) -> Tensor: ...
    def put_(
        self, index: Tensor, source: Tensor, accumulate: bool = False
    ) -> Tensor: ...
    def q_per_channel_axis(self) -> int: ...
    def q_per_channel_scales(self) -> Tensor: ...
    def q_per_channel_zero_points(self) -> Tensor: ...
    def q_scale(self) -> float: ...
    def q_zero_point(self) -> int: ...
    def qr(self, some: bool = True) -> ComplexReturnType("qr"): ...
    @dispatch
    def quantile(
        self,
        q: Tensor,
        dim: Optional[int] = None,
        keepdim: bool = False,
        *,
        interpolation: str = "linear",
    ) -> Tensor: ...
    @dispatch
    def quantile(
        self,
        q: float,
        dim: Optional[int] = None,
        keepdim: bool = False,
        *,
        interpolation: str = "linear",
    ) -> Tensor: ...
    def rad2deg(self) -> Tensor: ...
    def rad2deg_(self) -> Tensor: ...
    @dispatch
    def random_(self, *, generator: Optional[Generator] = None) -> Tensor: ...
    @dispatch
    def random_(
        self, from_: int, to: Optional[int], *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    @dispatch
    def random_(self, to: int, *, generator: Optional[Generator] = None) -> Tensor: ...
    def ravel(self) -> Tensor: ...
    def reciprocal(self) -> Tensor: ...
    def reciprocal_(self) -> Tensor: ...
    def refine_names(self, names: Sequence[Union[str, ellipsis, None]]) -> Tensor: ...
    def relu(self) -> Tensor: ...
    def relu_(self) -> Tensor: ...
    @dispatch
    def remainder(self, other: Tensor) -> Tensor: ...
    @dispatch
    def remainder(self, other: Number) -> Tensor: ...
    @dispatch
    def remainder_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def remainder_(self, other: Number) -> Tensor: ...
    def rename(
        self, names: Optional[Sequence[Union[str, ellipsis, None]]]
    ) -> Tensor: ...
    def rename_(
        self, names: Optional[Sequence[Union[str, ellipsis, None]]]
    ) -> Tensor: ...
    def renorm(self, p: Number, dim: int, maxnorm: Number) -> Tensor: ...
    def renorm_(self, p: Number, dim: int, maxnorm: Number) -> Tensor: ...
    @dispatch
    def repeat(self, repeats: List[int]) -> Tensor: ...
    @dispatch
    def repeat(self, *repeats: int) -> Tensor: ...
    @dispatch
    def repeat_interleave(
        self,
        repeats: Tensor,
        dim: Optional[int] = None,
        *,
        output_size: Optional[int] = None,
    ) -> Tensor: ...
    @dispatch
    def repeat_interleave(
        self,
        repeats: int,
        dim: Optional[int] = None,
        *,
        output_size: Optional[int] = None,
    ) -> Tensor: ...
    def requires_grad_(self, mode: bool = True) -> Tensor: ...
    @dispatch
    def reshape(self, shape: List[int]) -> Tensor: ...
    @dispatch
    def reshape(self, *shape: int) -> Tensor: ...
    def reshape_as(self, other: Tensor) -> Tensor: ...
    @dispatch
    def resize_(
        self, size: List[int], *, memory_format: Optional[memory_format] = None
    ) -> Tensor: ...
    @dispatch
    def resize_(
        self, *size: int, memory_format: Optional[memory_format] = None
    ) -> Tensor: ...
    def resize_as_(
        self, the_template: Tensor, *, memory_format: Optional[memory_format] = None
    ) -> Tensor: ...
    def resize_as_sparse_(self, the_template: Tensor) -> Tensor: ...
    def resolve_conj(self) -> Tensor: ...
    def resolve_neg(self) -> Tensor: ...
    def retain_grad(self) -> None: ...
    def roll(self, shifts: Union[int, Size], dims: Union[int, Size] = ()) -> Tensor: ...
    def rot90(self, k: int = 1, dims: Size = (0, 1)) -> Tensor: ...
    @dispatch
    def round(self) -> Tensor: ...
    @dispatch
    def round(self, *, decimals: int) -> Tensor: ...
    @dispatch
    def round_(self) -> Tensor: ...
    @dispatch
    def round_(self, *, decimals: int) -> Tensor: ...
    def row_indices(self) -> Tensor: ...
    def rsqrt(self) -> Tensor: ...
    def rsqrt_(self) -> Tensor: ...
    @dispatch
    def scatter(self, dim: int, index: Tensor, src: Tensor) -> Tensor: ...
    @dispatch
    def scatter(
        self, dim: int, index: Tensor, src: Tensor, *, reduce: str
    ) -> Tensor: ...
    @dispatch
    def scatter(
        self, dim: int, index: Tensor, value: Number, *, reduce: str
    ) -> Tensor: ...
    @dispatch
    def scatter(
        self, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor
    ) -> Tensor: ...
    @dispatch
    def scatter(self, dim: int, index: Tensor, value: Number) -> Tensor: ...
    @dispatch
    def scatter(
        self, dim: Union[str, ellipsis, None], index: Tensor, value: Number
    ) -> Tensor: ...
    @dispatch
    def scatter_(self, dim: int, index: Tensor, src: Tensor) -> Tensor: ...
    @dispatch
    def scatter_(
        self, dim: int, index: Tensor, src: Tensor, *, reduce: str
    ) -> Tensor: ...
    @dispatch
    def scatter_(
        self, dim: int, index: Tensor, value: Number, *, reduce: str
    ) -> Tensor: ...
    @dispatch
    def scatter_(self, dim: int, index: Tensor, value: Number) -> Tensor: ...
    @dispatch
    def scatter_add(self, dim: int, index: Tensor, src: Tensor) -> Tensor: ...
    @dispatch
    def scatter_add(
        self, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor
    ) -> Tensor: ...
    def scatter_add_(self, dim: int, index: Tensor, src: Tensor) -> Tensor: ...
    def scatter_reduce(
        self,
        dim: int,
        index: Tensor,
        src: Tensor,
        reduce: str,
        *,
        include_self: bool = True,
    ) -> Tensor: ...
    def scatter_reduce_(
        self,
        dim: int,
        index: Tensor,
        src: Tensor,
        reduce: str,
        *,
        include_self: bool = True,
    ) -> Tensor: ...
    @dispatch
    def select(self, dim: int, index: int) -> Tensor: ...
    @dispatch
    def select(self, dim: Union[str, ellipsis, None], index: int) -> Tensor: ...
    def select_scatter(self, src: Tensor, dim: int, index: int) -> Tensor: ...
    def sgn(self) -> Tensor: ...
    def sgn_(self) -> Tensor: ...
    def short(self) -> Tensor: ...
    def sigmoid(self) -> Tensor: ...
    def sigmoid_(self) -> Tensor: ...
    def sign(self) -> Tensor: ...
    def sign_(self) -> Tensor: ...
    def signbit(self) -> Tensor: ...
    def sin(self) -> Tensor: ...
    def sin_(self) -> Tensor: ...
    def sinc(self) -> Tensor: ...
    def sinc_(self) -> Tensor: ...
    def sinh(self) -> Tensor: ...
    def sinh_(self) -> Tensor: ...
    @dispatch
    def size(self) -> Size: ...
    @dispatch
    def size(self, dim: int) -> int: ...
    def slice_scatter(
        self,
        src: Tensor,
        dim: int = 0,
        start: Optional[int] = None,
        end: Optional[int] = None,
        step: int = 1,
    ) -> Tensor: ...
    def slogdet(self) -> ComplexReturnType("slogdet"): ...
    def smm(self, mat2: Tensor) -> Tensor: ...
    @dispatch
    def softmax(self, dim: int, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def softmax(
        self, dim: Union[str, ellipsis, None], *, dtype: Optional[pi_dtype] = None
    ) -> Tensor: ...
    @dispatch
    def sort(
        self, *, stable: Optional[bool], dim: int = -1, descending: bool = False
    ) -> ComplexReturnType("sort"): ...
    @dispatch
    def sort(
        self, dim: int = -1, descending: bool = False
    ) -> ComplexReturnType("sort"): ...
    @dispatch
    def sort(
        self,
        *,
        stable: Optional[bool],
        dim: Union[str, ellipsis, None],
        descending: bool = False,
    ) -> ComplexReturnType("sort"): ...
    @dispatch
    def sort(
        self, dim: Union[str, ellipsis, None], descending: bool = False
    ) -> ComplexReturnType("sort"): ...
    def sparse_dim(self) -> int: ...
    def sparse_mask(self, mask: Tensor) -> Tensor: ...
    def sparse_resize_(self, size: Size, sparse_dim: int, dense_dim: int) -> Tensor: ...
    def sparse_resize_and_clear_(
        self, size: Size, sparse_dim: int, dense_dim: int
    ) -> Tensor: ...
    @dispatch
    def split(self, split_size: int, dim: int = 0) -> Sequence[Tensor]: ...
    @dispatch
    def split(self, split_size: Tuple[int, ...], dim: int = 0) -> Sequence[Tensor]: ...
    def split_with_sizes(
        self, split_sizes: List[int], dim: int = 0
    ) -> List[Tensor]: ...
    def sqrt(self) -> Tensor: ...
    def sqrt_(self) -> Tensor: ...
    def square(self) -> Tensor: ...
    def square_(self) -> Tensor: ...
    @dispatch
    def squeeze(self) -> Tensor: ...
    @dispatch
    def squeeze(self, dim: int) -> Tensor: ...
    @dispatch
    def squeeze(self, dim: Union[str, ellipsis, None]) -> Tensor: ...
    @dispatch
    def squeeze_(self) -> Tensor: ...
    @dispatch
    def squeeze_(self, dim: int) -> Tensor: ...
    @dispatch
    def squeeze_(self, dim: Union[str, ellipsis, None]) -> Tensor: ...
    def sspaddmm(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number = 1, alpha: Number = 1
    ) -> Tensor: ...
    @dispatch
    def std(
        self,
        dim: Optional[Union[int, Size]],
        unbiased: bool = True,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def std(
        self,
        dim: Optional[Union[int, Size]] = None,
        *,
        correction: Optional[int] = None,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def std(self, unbiased: bool = True) -> Tensor: ...
    @dispatch
    def std(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        unbiased: bool = True,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def std(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        *,
        correction: Optional[int] = None,
        keepdim: bool = False,
    ) -> Tensor: ...
    def storage_offset(self) -> int: ...
    @dispatch
    def stride(self) -> Tuple[int, ...]: ...
    @dispatch
    def stride(self, _int) -> int: ...
    def sub(
        self,
        other: Union[Tensor, Number],
        *,
        alpha: Optional[Number] = 1,
        out: Optional[Tensor] = None,
    ) -> Tensor: ...
    def sub_(
        self, other: Union[Tensor, Number], *, alpha: Optional[Number] = 1
    ) -> Tensor: ...
    @dispatch
    def subtract(self, other: Tensor, *, alpha: Number = 1) -> Tensor: ...
    @dispatch
    def subtract(self, other: Number, alpha: Number = 1) -> Tensor: ...
    @dispatch
    def subtract_(self, other: Tensor, *, alpha: Number = 1) -> Tensor: ...
    @dispatch
    def subtract_(self, other: Number, alpha: Number = 1) -> Tensor: ...
    @dispatch
    def sum(self, *, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    @dispatch
    def sum(
        self,
        dim: Optional[Union[int, Size]],
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def sum(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        keepdim: bool = False,
        *,
        dtype: Optional[pi_dtype] = None,
    ) -> Tensor: ...
    @dispatch
    def sum_to_size(self, size: Size) -> Tensor: ...
    @dispatch
    def sum_to_size(self, *size: int) -> Tensor: ...
    def svd(
        self, some: bool = True, compute_uv: bool = True
    ) -> ComplexReturnType("svd"): ...
    def swapaxes(self, axis0: int, axis1: int) -> Tensor: ...
    def swapaxes_(self, axis0: int, axis1: int) -> Tensor: ...
    def swapdims(self, dim0: int, dim1: int) -> Tensor: ...
    def swapdims_(self, dim0: int, dim1: int) -> Tensor: ...
    def symeig(
        self, eigenvectors: bool = False, upper: bool = True
    ) -> ComplexReturnType("symeig"): ...
    def t(self) -> Tensor: ...
    def t_(self) -> Tensor: ...
    def take(self, index: Tensor) -> Tensor: ...
    def take_along_dim(self, indices: Tensor, dim: Optional[int] = None) -> Tensor: ...
    def tan(self) -> Tensor: ...
    def tan_(self) -> Tensor: ...
    def tanh(self) -> Tensor: ...
    def tanh_(self) -> Tensor: ...
    @dispatch
    def tensor_split(self, indices: List[int], dim: int = 0) -> List[Tensor]: ...
    @dispatch
    def tensor_split(
        self, tensor_indices_or_sections: Tensor, dim: int = 0
    ) -> List[Tensor]: ...
    @dispatch
    def tensor_split(self, sections: int, dim: int = 0) -> List[Tensor]: ...
    @dispatch
    def tile(self, dims: Size) -> Tensor: ...
    @dispatch
    def tile(self, *dims: int) -> Tensor: ...
    @dispatch
    def to(
        self, dtype: pi_dtype, non_blocking: bool = False, copy: bool = False
    ) -> Tensor: ...
    @dispatch
    def to(
        self,
        device: Optional[Union[device, str]] = None,
        dtype: Optional[pi_dtype] = None,
        non_blocking: bool = False,
        copy: bool = False,
    ) -> Tensor: ...
    @dispatch
    def to(
        self, other: Tensor, non_blocking: bool = False, copy: bool = False
    ) -> Tensor: ...
    def to_dense(self, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    def to_mkldnn(self, dtype: Optional[pi_dtype] = None) -> Tensor: ...
    def to_padded_tensor(
        self, padding: float, output_size: Optional[List[int]] = None
    ) -> Tensor: ...
    @dispatch
    def to_sparse(
        self,
        *,
        layout: Optional[layout] = None,
        blocksize: Optional[Union[int, Size]] = None,
    ) -> Tensor: ...
    @dispatch
    def to_sparse(self, sparse_dim: int) -> Tensor: ...
    @dispatch
    def to_sparse_bsc(self, blocksize: Union[int, Size]) -> Tensor: ...
    @dispatch
    def to_sparse_bsc(self, *blocksize: int) -> Tensor: ...
    @dispatch
    def to_sparse_bsr(self, blocksize: Union[int, Size]) -> Tensor: ...
    @dispatch
    def to_sparse_bsr(self, *blocksize: int) -> Tensor: ...
    def to_sparse_csc(self) -> Tensor: ...
    def to_sparse_csr(self) -> Tensor: ...
    def tolist(self) -> List: ...
    def topk(
        self, k: int, dim: int = -1, largest: bool = True, sorted: bool = True
    ) -> ComplexReturnType("topk"): ...
    def trace(self) -> Tensor: ...
    @dispatch
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    @dispatch
    def transpose(
        self, dim0: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None]
    ) -> Tensor: ...
    def transpose_(self, dim0: int, dim1: int) -> Tensor: ...
    def triangular_solve(
        self,
        A: Tensor,
        upper: bool = True,
        transpose: bool = False,
        unitriangular: bool = False,
    ) -> ComplexReturnType("triangular_solve"): ...
    def tril(self, diagonal: int = 0) -> Tensor: ...
    def tril_(self, diagonal: int = 0) -> Tensor: ...
    def triu(self, diagonal: int = 0) -> Tensor: ...
    def triu_(self, diagonal: int = 0) -> Tensor: ...
    def true_divide(
        self, other: Union[Tensor, Number], *, out: Optional[Tensor] = None
    ) -> Tensor: ...
    def true_divide_(self, other: Union[Tensor, Number]) -> Tensor: ...
    def trunc(self) -> Tensor: ...
    def trunc_(self) -> Tensor: ...
    @dispatch
    def unbind(self, dim: int = 0) -> List[Tensor]: ...
    @dispatch
    def unbind(self, dim: Union[str, ellipsis, None]) -> List[Tensor]: ...
    @dispatch
    def unflatten(
        self,
        dim: Union[str, ellipsis, None],
        sizes: Size,
        names: Sequence[Union[str, ellipsis, None]],
    ) -> Tensor: ...
    @dispatch
    def unflatten(self, dim: int, sizes: Size) -> Tensor: ...
    def unfold(self, dimension: int, size: int, step: int) -> Tensor: ...
    def uniform_(
        self, from_: float = 0, to: float = 1, *, generator: Optional[Generator] = None
    ) -> Tensor: ...
    def unsafe_chunk(self, chunks: int, dim: int = 0) -> List[Tensor]: ...
    def unsafe_split(self, split_size: int, dim: int = 0) -> List[Tensor]: ...
    def unsafe_split_with_sizes(
        self, split_sizes: List[int], dim: int = 0
    ) -> List[Tensor]: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def unsqueeze_(self, dim: int) -> Tensor: ...
    def values(self) -> Tensor: ...
    @dispatch
    def var(
        self,
        dim: Optional[Union[int, Size]],
        unbiased: bool = True,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def var(
        self,
        dim: Optional[Union[int, Size]] = None,
        *,
        correction: Optional[int] = None,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def var(self, unbiased: bool = True) -> Tensor: ...
    @dispatch
    def var(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        unbiased: bool = True,
        keepdim: bool = False,
    ) -> Tensor: ...
    @dispatch
    def var(
        self,
        dim: Sequence[Union[str, ellipsis, None]],
        *,
        correction: Optional[int] = None,
        keepdim: bool = False,
    ) -> Tensor: ...
    def vdot(self, other: Tensor) -> Tensor: ...
    @dispatch
    def view(self, dtype: pi_dtype) -> Tensor: ...
    @dispatch
    def view(self, size: List[int]) -> Tensor: ...
    @dispatch
    def view(self, *size: int) -> Tensor: ...
    def view_as(self, other: Tensor) -> Tensor: ...
    @dispatch
    def vsplit(self, sections: int) -> List[Tensor]: ...
    @dispatch
    def vsplit(self, indices: Size) -> List[Tensor]: ...
    @dispatch
    def vsplit(self, *indices: int) -> List[Tensor]: ...
    def where(self, condition: Tensor, other: Tensor) -> Tensor: ...
    @dispatch
    def xlogy(self, other: Tensor) -> Tensor: ...
    @dispatch
    def xlogy(self, other: Number) -> Tensor: ...
    @dispatch
    def xlogy_(self, other: Tensor) -> Tensor: ...
    @dispatch
    def xlogy_(self, other: Number) -> Tensor: ...
    def zero_(self) -> Tensor: ...

def from_numpy(arr: np.ndarray):
    from pi import DEBUG

    if DEBUG:
        arr = np.ones_like(arr, dtype=np.float32)
    attr = DenseElementsAttr.get(arr)
    vt = Tensor(torch_dialect.ValueTensorLiteralOp(attr))
    return vt

def empty(shape: Tuple[int, ...], dtype: "pi.dtype" = None, **kwargs) -> Tensor:
    if np.prod(shape) == 0:
        return Tensor(None)
    elif dtype is not None:
        dtype = dtype.to_np_type()
    return from_numpy(np.empty(shape, dtype))

def randint(low: int, high: int, size: Tuple[int, ...]) -> Tensor:
    return from_numpy(np.random.randint(low, high, size))

def randn(*size: Tuple[int, ...]) -> Tensor:
    if size == ((),):
        return from_numpy(np.random.randn())
    return from_numpy(np.random.randn(*size))

def uniform(low: float, high: float, size: Tuple[int, ...]) -> Tensor:
    return from_numpy(np.random.uniform(low, high, size))

def rand(*size: Tuple[int, ...], **kwargs) -> Tensor:
    if size == ((),):
        return from_numpy(np.random.rand())
    return from_numpy(np.random.rand(*size))

def ones(*size: Tuple[int, ...], **kwargs) -> Tensor:
    dtype = kwargs.get("dtype", None)
    if dtype is not None:
        dtype = dtype.to_np_type()
    return from_numpy(np.ones(size, dtype=dtype))

def zeros(*size: Tuple[int, ...], **kwargs) -> Tensor:
    dtype = kwargs.get("dtype", None)
    if dtype is not None:
        dtype = dtype.to_np_type()
    return from_numpy(np.zeros(size, dtype))

def tensor(data: Any, dtype: Optional["pi.dtype"] = None) -> Tensor:
    if dtype is not None:
        dtype = dtype.to_np_type()
    return from_numpy(np.array(data, dtype=dtype))

def LongTensor(data: Any) -> Tensor:
    return from_numpy(np.array(data, dtype=pi_dtype.int64.to_np_type()))

def clone(x: Tensor, **kwargs):
    warnings.warn(f"not actually cloning")
    return x

__all__ = [
    "from_numpy",
    "empty",
    "randint",
    "randn",
    "rand",
    "uniform",
    "ones",
    "tensor",
    "Tensor",
    "LongTensor",
    "zeros",
]
